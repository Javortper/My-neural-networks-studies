{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Keras y Mnist BÁSICO.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"An6_AoGVjhwm","colab_type":"text"},"source":["#Introducción a Keras"]},{"cell_type":"markdown","metadata":{"id":"_o0ll8rWj8Gr","colab_type":"text"},"source":["Antes de meternos de lleno con el problema a resolver pienso que sería interesante añadir un ejemplo introductorio a la librería Keras. En vista de que este TFG no es sólo de resolver un problema sino un acercamiento práctico al Deep Learning y de que varias de las prácticas que haremos ahora se realizarán a la hora de resolver el problema, veo necesario (tanto para este trabajo como para mi)  este apartado.\n","\n","Por lo tanto en este apéndice veremos como:\n","\n","\n","1.   Importar un conjunto de datos de Keras, en concreto MNIST.\n","2.   Procesado del dataset para poder usarlo en nuestro modelo.\n","3.   Generar y entrenar un modelo\n","4.   Realizar clasificaciones.\n","\n","\n","\n","\n","\n","[Guía seguida](https://nextjournal.com/gkoehler/digit-recognition-with-keras)"]},{"cell_type":"markdown","metadata":{"id":"vG1X1gzA7EYd","colab_type":"text"},"source":["##Preprocesado del conjunto de datos"]},{"cell_type":"code","metadata":{"id":"TCQkjEi-e8uz","colab_type":"code","colab":{}},"source":["# keras imports for the dataset and building our neural network\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers.core import Dense, Dropout, Activation\n","from keras.utils import np_utils\n","import matplotlib.pyplot as plt\n","#Para la distribución\n","from collections import Counter\n","import seaborn as sns \n","\n","\n","#Instanciamos el dataset\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pw5ZXl3dlLei","colab_type":"text"},"source":["\n","Como sabemos MNIST está formada por imágenes de 28x28 en escala de grises. Veamos a continuación el formato de las imágenes del Dataset que nos propone Keras.\n"]},{"cell_type":"code","metadata":{"id":"3UG3_zgVldCx","colab_type":"code","outputId":"574597ad-9d69-4e40-8533-8364476968ae","executionInfo":{"status":"ok","timestamp":1562767559119,"user_tz":-120,"elapsed":1452,"user":{"displayName":"Javi Ortiz","photoUrl":"","userId":"07381208674183285344"}},"colab":{"base_uri":"https://localhost:8080/","height":674}},"source":["print(\"Número de imágenes de train: \", len(X_train))\n","print(\"Número de imágenes de test: \", len(X_test))\n","print(\"Formato de imagen individual: \", X_train[0].shape)\n","print(\"Formato del conjunto entero: \", X_train.shape, \"Es decir, 60000 imágenes de 28x28 p \\n\")\n","\n","#Mostramos una imagen de entrenamiento de ejemplo\n","print(\"Figura de ejemplo:\")\n","plt.imshow(X_train[0])\n","plt.show()\n","\n","print(\"Accedemos a su posición en el vector de etiquetas:\", y_train[0], \"\\n\")\n","\n","#Mostramos la distribución\n","print(\"Distribución de clases de entrenamiento:\")\n","distr = Counter(y_train)\n","plt.bar(distr.keys(), distr.values())\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Número de imágenes de train:  60000\n","Número de imágenes de test:  10000\n","Formato de imagen individual:  (28, 28)\n","Formato del conjunto entero:  (60000, 28, 28) Es decir, 60000 imágenes de 28x28 p \n","\n","Figura de ejemplo:\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Accedemos a su posición en el vector de etiquetas: 5 \n","\n","Distribución de clases de entrenamiento:\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"cKPS-1jZsyJI","colab_type":"text"},"source":["Si nos fijamos en la tabla de **distribución** vemos que más o menos hay un mismo número de ejemplos para cada una de las clases. Es importante tener un conjunto de datos equilibrado para que nuestra red neuronal trabaje correctamente.\n","\n","Ahora realizaremos una **normalización** entre 0 y 1 de nuestros datos de entrenamiento. Citando un post de StackOverflow [[1]](https://stackoverflow.com/questions/51593082/need-of-normalizing-data/51601642):\n","\n","\n","> `Normalization has nothing to do with the training speed, normalized data permit to the model to learn the importance and the correlation between the features. The most of the time a \"non-normalized\" data don't permit a correct approximation of the input, resulting in strange results.`\n","\n","\n","También es importante formatear las etiquetas del conjunto de entrenamiento (digitos de 0 a 9) usado **one-hot encoding**. El resultado será que una etiqueta (o clasificación para un ejemplo) en vez de ser un número será un vector de diez digitos con todos ceros excepto un uno donde corresponda la clasificación. Para esto hacemos uso de np.utils de Keras\n","\n","También es importante tener en cuenta que **los datos de entrada de la red convolucional deben de estar bien formateados** . En nuestro caso las redes de keras tienen dos formatos:\n","\n","\n","1.   first_channel: el primer valor es el número de canales que tiene la imagen (channels, rows, colums) -> (1, 28, 28)\n","2.   last_channel: (28, 28, 1)\n","\n","En el apartado anterior vimos que las imagenes del dataset tienen formato (28, 28) por lo tanto tendremos que añadirle a la tupla otro elemento que diga el número de canales que tiene nuestra imagen.\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"4PoX_ia3u8Mn","colab_type":"code","outputId":"fe9d9f25-f77c-4079-98c8-d1c73936b1a8","executionInfo":{"status":"ok","timestamp":1562767562836,"user_tz":-120,"elapsed":1335,"user":{"displayName":"Javi Ortiz","photoUrl":"","userId":"07381208674183285344"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["#Formateamos las imágenes\n","print(\"Formato anterior de las imágenes:\", X_train.shape)\n","X_train = X_train.reshape(60000, 28, 28, 1)\n","print(\"Nuevo formato para las imágenes:\", X_train.shape)\n","X_test = X_test.reshape(10000, 28, 28, 1)\n","X_train = X_train.astype('float32') #Esto lo hacemos para poder aplicar en el siguiente paso /=\n","X_test = X_test.astype('float32')\n","\n","\n","#Normalización de los datos\n","X_train /= 255\n","X_test /= 255\n","\n","#One-hot encoding\n","n_clases = 10\n","Y_train = np_utils.to_categorical(y_train, n_clases)\n","Y_test = np_utils.to_categorical(y_test, n_clases)\n","\n","print(\"\\nPara la imagen anteriormente mostrada (dígito 5) la clasificación sería:\")\n","print(\"Clasificación que viene de serie:\", y_train[0])\n","print(\"Clasificación con formato en one-hot:\", Y_train[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Formato anterior de las imágenes: (60000, 28, 28)\n","Nuevo formato para las imágenes: (60000, 28, 28, 1)\n","\n","Para la imagen anteriormente mostrada (dígito 5) la clasificación sería:\n","Clasificación que viene de serie: 5\n","Clasificación con formato en one-hot: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-HbshzxN1Vpc","colab_type":"text"},"source":["##Generando la red neuronal"]},{"cell_type":"markdown","metadata":{"id":"lCytUJNm5ltl","colab_type":"text"},"source":["Para generar la red neuronal usaremos diversas clases para cada una de las capas. Usaremos un modelo de tipo **Sequential**, este nos permitirá hacer un modelo capa a capa. Usaremos **add()** para añadir capas a nuestro modelo. Viendo como funciona una red convolucional usaremos estos tipos de capas:\n","\n","\n","*   Convolucional -> Conv2D(nº filtros, tamaño kernel, funcion de activacion, margen, stride, formato de entrada)\n","*   Pooling -> MaxPooling2D(tamaño del kernel)\n","\n","*   Capa plana -> Flatten()\n","*   Capa conectada hacia delante para clasificacion -> Dense(n1 de caslses, función activación)\n","\n","\n","\n","\n","Para este ejemplo haremos uso de la red **VGG13** la cual sigue la sigueinte estructura: \n","\n","![texto alternativo](https://cdn-images-1.medium.com/max/800/1*zOjMs185kZmqh2jK5kRp_Q.png)\n","\n","\n","Nota; cada convolución tiene un kernel de 3x3 con stride 1 (número de casillas que se desplaza), padding = same y las capas de pooling usan un kernel de 2x2, stride=2 y padding=same"]},{"cell_type":"code","metadata":{"id":"Dm_3a8IShkIk","colab_type":"code","outputId":"2d6879eb-243b-4f3c-83f6-54ddc57724cc","executionInfo":{"status":"ok","timestamp":1562767566494,"user_tz":-120,"elapsed":1465,"user":{"displayName":"Javi Ortiz","photoUrl":"","userId":"07381208674183285344"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#Importaciones\n","from keras.models import Sequential\n","from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n","\n","modelo = Sequential()\n","#Debemos meter el formato de entrada solo en la primera capa\n","modelo.add(Conv2D(64, kernel_size=(3,3), padding='same', strides=(1,1), activation ='relu', input_shape=(28,28,1)))\n","modelo.add(Conv2D(64, kernel_size=(3,3), padding='same', strides=(1,1), activation ='relu'))\n","modelo.add(MaxPooling2D(pool_size=(2,2), padding=\"same\", strides=(2,2)))\n","\n","modelo.add(Conv2D(128, kernel_size=(3,3), padding='same', strides=(1,1), activation ='relu'))\n","modelo.add(Conv2D(128, kernel_size=(3,3), padding='same', strides=(1,1), activation ='relu'))\n","modelo.add(MaxPooling2D(pool_size=(2,2), padding=\"same\", strides=(2,2)))\n","\n","modelo.add(Conv2D(256, kernel_size=(3,3), padding='same', strides=(1,1), activation ='relu'))\n","modelo.add(Conv2D(256, kernel_size=(3,3), padding='same', strides=(1,1), activation ='relu'))\n","modelo.add(MaxPooling2D(pool_size=(2,2), padding=\"same\", strides=(2,2)))\n","\n","modelo.add(Conv2D(512, kernel_size=(3,3), padding='same', strides=(1,1), activation ='relu'))\n","modelo.add(Conv2D(512, kernel_size=(3,3), padding='same', strides=(1,1), activation ='relu'))\n","modelo.add(MaxPooling2D(pool_size=(2,2), padding=\"same\", strides=(2,2)))\n","\n","modelo.add(Conv2D(512, kernel_size=(3,3), padding='same', strides=(1,1), activation ='relu'))\n","modelo.add(Conv2D(512, kernel_size=(3,3), padding='same', strides=(1,1), activation ='relu'))\n","modelo.add(MaxPooling2D(pool_size=(2,2), padding=\"same\", strides=(2,2)))\n","\n","modelo.add(Flatten())\n","modelo.add(Dense(4096, activation='relu'))\n","modelo.add(Dropout(0.5))\n","modelo.add(Dense(4096, activation='relu'))\n","modelo.add(Dropout(0.5))\n","modelo.add(Dense(10, activation='softmax'))\n","\n","\n","#Ahora vamos a compilar el modelo creado asignadole las funciones pérdida, de optimizacién y las métricas que queramos usar\n","modelo.compile(loss='categorical_crossentropy', \n","         optimizer='sgd',\n","         metrics=['accuracy'])\n","\n","#Mostramos la estructura del modelo\n","modelo.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0710 14:06:05.400171 140045549115264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0710 14:06:05.458263 140045549115264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0710 14:06:05.470842 140045549115264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0710 14:06:05.520175 140045549115264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","W0710 14:06:05.678893 140045549115264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","W0710 14:06:05.692159 140045549115264 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","W0710 14:06:05.763963 140045549115264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0710 14:06:05.791480 140045549115264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1 (Conv2D)            (None, 28, 28, 64)        640       \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 28, 28, 64)        36928     \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 14, 14, 128)       73856     \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 14, 14, 128)       147584    \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 7, 7, 128)         0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 7, 7, 256)         295168    \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 7, 7, 256)         590080    \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 4, 4, 512)         1180160   \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 4, 4, 512)         2359808   \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 2, 2, 512)         0         \n","_________________________________________________________________\n","conv2d_9 (Conv2D)            (None, 2, 2, 512)         2359808   \n","_________________________________________________________________\n","conv2d_10 (Conv2D)           (None, 2, 2, 512)         2359808   \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 1, 1, 512)         0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 512)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 4096)              2101248   \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 4096)              0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 4096)              16781312  \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 4096)              0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 10)                40970     \n","=================================================================\n","Total params: 28,327,370\n","Trainable params: 28,327,370\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5gl0YRbKo8H7","colab_type":"text"},"source":["##Entrenando el modelo"]},{"cell_type":"markdown","metadata":{"id":"j_VzWcRPCQFD","colab_type":"text"},"source":["Entrenar nuestro modelo es muy facil usando Keras. Para ello usaremos la función **fit()**.\n","\n","\n","\n","```\n","model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          verbose=1,\n","          validation_data=(x_test, y_test))\n","```\n","\n","\n","*   **batch_size**: número de ejemplos que se pasarán a la red neuronal de una vez. Cuanto mas grande es el batch mas rápido entrenará alcanzará un óptimo (más rapido = menos iteraciones, pero no tiene porque ser menos tiempo) pero más potencia de cómputo es necesaria ya que intentará procesar un mayor número de imágenes de una vez.  \n","*   **epochs**: número que define el número de veces que nuestro algoritmo de aprendizaje recorrera **todo** el conjunto de entrenamiento. Si no se indica nada se pondrá a 32 por defecto.\n","\n","*   **verbose**: nos muestra graficamente como va el entrenamiento. 0=no muestra nada, 1=barra de progreso, 2=una linea por cada epoch.\n","*   **validation_data**: si pasamos datos aquí serán usados para evaluar la función perdida y las metricas al final de cada epoch.\n","\n","Ejemplo: Si ponemos batch_size = 50 y tenemos 1000 imágenes, se hara uso de 20 batches para completar un epoch. \n","\n","Otros parámetros utiles...:\n","steps_per_epoch\n","shuffle\n","\n","\n","**Ventajas de usar un batch size < numero de ejemplos:**\n","\n","*   Se requiere una memoria menor. \n","*   La red entrenará más rapido. Esto es debido a que los pesos se actualizan tras cada propagación, es decir, cada batch.\n","\n","**Desventajas:**\n","*   Un batch menor dará una precisión menor a la hora de estimar el gradiente.\n","\n","Más información: [Stackoverflow: What is batch size in neural network?(https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network)\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"QHTLMlotLajy","colab_type":"code","colab":{}},"source":["print(len(Y_test))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ysDVcW4lJ-gh","colab_type":"code","outputId":"21458e20-390e-4a54-e099-84eddd267ede","executionInfo":{"status":"ok","timestamp":1562768676555,"user_tz":-120,"elapsed":1104477,"user":{"displayName":"Javi Ortiz","photoUrl":"","userId":"07381208674183285344"}},"colab":{"base_uri":"https://localhost:8080/","height":462}},"source":["#Entrenamos\n","batch_size = 32\n","epochs = 10\n","modelo.fit(X_train, Y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          verbose=1,\n","          validation_data=(X_test, Y_test))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["W0710 14:06:12.541396 140045549115264 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 60000 samples, validate on 10000 samples\n","Epoch 1/10\n","60000/60000 [==============================] - 117s 2ms/step - loss: 2.2991 - acc: 0.1122 - val_loss: 2.2895 - val_acc: 0.1135\n","Epoch 2/10\n","60000/60000 [==============================] - 110s 2ms/step - loss: 1.3414 - acc: 0.5290 - val_loss: 0.1787 - val_acc: 0.9456\n","Epoch 3/10\n","60000/60000 [==============================] - 110s 2ms/step - loss: 0.1454 - acc: 0.9550 - val_loss: 0.0869 - val_acc: 0.9722\n","Epoch 4/10\n","60000/60000 [==============================] - 110s 2ms/step - loss: 0.0829 - acc: 0.9741 - val_loss: 0.0536 - val_acc: 0.9826\n","Epoch 5/10\n","60000/60000 [==============================] - 110s 2ms/step - loss: 0.0596 - acc: 0.9815 - val_loss: 0.0500 - val_acc: 0.9837\n","Epoch 6/10\n","60000/60000 [==============================] - 110s 2ms/step - loss: 0.0464 - acc: 0.9857 - val_loss: 0.0419 - val_acc: 0.9874\n","Epoch 7/10\n","60000/60000 [==============================] - 110s 2ms/step - loss: 0.0356 - acc: 0.9893 - val_loss: 0.0421 - val_acc: 0.9867\n","Epoch 8/10\n","60000/60000 [==============================] - 109s 2ms/step - loss: 0.0301 - acc: 0.9902 - val_loss: 0.0334 - val_acc: 0.9889\n","Epoch 9/10\n","60000/60000 [==============================] - 109s 2ms/step - loss: 0.0253 - acc: 0.9918 - val_loss: 0.0393 - val_acc: 0.9887\n","Epoch 10/10\n","60000/60000 [==============================] - 109s 2ms/step - loss: 0.0196 - acc: 0.9943 - val_loss: 0.0347 - val_acc: 0.9890\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f5ec3577828>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"zHAmZUHVSLph","colab_type":"text"},"source":["##Probando y guardando el modelo "]},{"cell_type":"markdown","metadata":{"id":"TGgc4n6pdggm","colab_type":"text"},"source":["A continuación probaremos a realizar una clasificación con nuestra red entrenada. Para hacer esto haremos uso del comando **predict()**.  Este comando genera una predicción para las entradas que recibe. La entrada debe ser un array con el mismo formato que recibe la red neuronal. En este caso la red neuronal tiene de entreada el formato (i, x, y, r) donde i indica el numero de elementos del array, x las filas (28), y las columnas (28) y r los canales que como es en escala de grises r = 1.\n","\n","Por lo tanto si queremos predecir UNA sola imagen deberá tomar el formato (1, 28, 28, 1). A continuación elegiremos una imagen del conjunto test y lo formatearemos con el comando reshape(). Una vez hecho esto podremos introducirla en el predict(). Lo que devolverá será un array de longitud igual al número de clases que hay y para cada una una estimación de que pertenezca o no a esa clase."]},{"cell_type":"code","metadata":{"id":"sJp42B85duiP","colab_type":"code","outputId":"79c5c41b-a2bc-4320-e6a3-0bbad8e7b5e7","executionInfo":{"status":"ok","timestamp":1562776960114,"user_tz":-120,"elapsed":1073,"user":{"displayName":"Javi Ortiz","photoUrl":"","userId":"07381208674183285344"}},"colab":{"base_uri":"https://localhost:8080/","height":286}},"source":["#Imagen a clasificar\n","img_prueba = X_test[0] \n","clas = Y_test[0]\n","\n","#Para poder mostrarla con imshow() necesitamos un array bidimensional y actualmente su formato es (28,28,1) \n","prueba = prueba.reshape(28, 28)\n","plt.imshow(prueba)\n","plt.show()\n","\n","#Vemos que vamos a clasificar un 7. Veamos que nos devuelve la red neuronal\n","prueba = prueba.reshape(1, 28, 28, 1)\n","clasificacion = list(modelo.predict(prueba)[0])\n","resultado = clasificacion.index(max(clasificacion))\n","print(\"La red neuronal determina que la imagen es un: \", resultado)\n","\n"," "],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["La red neuronal determina que la imagen es un:  7\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ASiiMwIvwzFx","colab_type":"text"},"source":["###Guardar y cargar un modelo\n","Para guardar el modelo haremos uso del método save() y para cargarlo el método load_model(). El tipo de fichero que usa Keras es del formato .h5"]},{"cell_type":"code","metadata":{"id":"ww1M70Pbw7mX","colab_type":"code","colab":{}},"source":["#Guardamos el modelo\n","modelo.save('VGG13_MNIST_model.h5')\n","\n","#Si quisieramos cargarlo\n","#model = load_model('my_model.h5')"],"execution_count":0,"outputs":[]}]}