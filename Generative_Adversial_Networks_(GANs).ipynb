{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generative Adversial Networks (GANs).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Javortper/Neural-Networks/blob/master/Generative_Adversial_Networks_(GANs).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmFH_Jd-Y8fz",
        "colab_type": "text"
      },
      "source": [
        "#Introducción del notebook\n",
        "\n",
        "En este Notebook recogeré información teórica sobre las GANs. Mi idea es entender bien como funcionan y a partir de ahi intentar implementar una por mi cuenta. \n",
        "\n",
        "Recursos usados:\n",
        "\n",
        "[A Gentle Introduction to Generative Adversarial Networks ](https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/)\n",
        "\n",
        "[A Beginner's Guide to Generative Adversarial Networks (GANs)](https://skymind.ai/wiki/generative-adversarial-network-gan)\n",
        "\n",
        "[Understanding Generative Adversarial Networks (GANs)](https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29)\n",
        "\n",
        "[Generative Adversarial Network(GAN) using Keras](https://medium.com/datadriveninvestor/generative-adversarial-network-gan-using-keras-ce1c05cfdfd3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muBvKFxGxokG",
        "colab_type": "text"
      },
      "source": [
        "#Parte teórica: fundamentos de las GANs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chtNsEBrdu2y",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##¿Que son los Generative Adversarial Networks?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zymnx9EZjSV",
        "colab_type": "text"
      },
      "source": [
        "Los Generative Adversarial Networks (GANs) o redes generativas antagónicas en español, son modelos de Deep Learning basados en modelos generadores. Las GANs son modelos con arquitecturas para entrenar un modelo generador y la mayoria de veces se hace uso de modelos de Deep Learning para esta arquitectura.\n",
        "\n",
        "La primera arquitectura GAN se mostró en 2014 en el paper de Ian Goodfellow titulado [\"Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)\n",
        "\n",
        "Un modelo GAN hace uso de dos submodelos. Un modelo **generador** para crear nuevos ejemplos y un modelo **discriminador** para clasificar ejemplos como reales o falsos (generados por el generator).\n",
        "\n",
        "```\n",
        "*Generative adversarial networks are based on a game theoretic scenario in which the generator network must compete against an adversary. The generator network directly produces samples. Its adversary, the discriminator network, attempts to distinguish between samples drawn from the training data and samples drawn from the generator.*\n",
        "```\n",
        "-Cita del libro Deep Learning, página 699. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKQJNv1MwYo6",
        "colab_type": "text"
      },
      "source": [
        "## ¿Cómo funcionan las GANs?\n",
        "\n",
        "El modelo **generador** crea nuevos ejemplos de datos mientras que el **discriminador** evalua su autenticidad. Es decir, el generador busca engañar sin ser pillado y el el discriminador intenta identificar aquellas imágenes que vienen del generador como falsas.\n",
        "\n",
        "Los pasos que hace un GAN son:\n",
        "1.   El generador toma un número aleatorio y devuelve una imagen.\n",
        "2.   La imagen generada se pasa al discriminador junto  a una secuencia de imágenes reales.\n",
        "3.   El disciminador clasifica como reales o falsas y devuelve probabilidades entre 0 y 1, siendo 1 predicción de aunténtica y 0 falsa.\n",
        "\n",
        "Para el caso de MNIST, la red discrimandora es una red convolucional estandar que puede clasificar imagenes como reales o falsas. El generador es basicamente lo inverso a una red convolucional, mientras que la convolucional toma imágenes y le hace downsampling (capa de Pooling) obtener una probabilidad, el generador toma un vector de ruido aleatorio y le aplica \"upsampling\" para obtener una imagen.\n",
        "\n",
        "Las dos redes buscan optimizar diferentes funciones objetivos o funciones de perdida. Funcionando como un [juego de suma cero](https://es.wikipedia.org/wiki/Juego_de_suma_cero). Esto es basicamente un **actor-critic model**(buscar información), donde cambios en el discriminador provoca cambios en el generador y viceversa. \n",
        "\n",
        "![Ejemplo de GAN funcionando con MNIST. Recurso tomado de https://www.freecodecamp.org/news/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394/](https://cdn-media-1.freecodecamp.org/images/m41LtQVUf3uk5IOYlHLpPazxI3pWDwG8VEvU)\n",
        "\n",
        "En definitiva:\n",
        "\n",
        "\n",
        "*   El generador busca maximizar la probabilidad de que el discriminador falle.\n",
        "*   El disciminador guía al generador para producir imágenes mas realistas.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGDKyvmMRhk6",
        "colab_type": "text"
      },
      "source": [
        "##Profundizando en el funcionamiento\n",
        "\n",
        "###El modelo generador\n",
        "\n",
        "[Información tomada de este artículo](https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/).\n",
        "\n",
        "El generador toma como entrada una variable aleatoria y genera una muestra dentro del dominion.\n",
        "\n",
        "El vector se genera aleatoriamente a partir de una distribución Gaussiana. Este vector alimenta el proceso de generar una imagen. Despues del entrenamiento, los puntos del espacio de este vector multidimensional corresponderan con puntos dentro del dominio del problema, formando una representación comprimida de la distribución de la información.\n",
        "\n",
        "Este vector espacio se denomina como \"latent space\" o \"vector space comprised of [latent variables](https://en.wikipedia.org/wiki/Latent_variable). Latent variables o variables ocultas son unas variables importantes para el dominio pero que no son observables directamente.\n",
        "\n",
        "Muchas veces nos referimos a estas latent variables o latent space como una proyección o compresión de una distribución de datos. \n",
        "\n",
        "```\n",
        "\"That is, a latent space provides a compression or high-level concepts of the observed raw data such as the input data distribution. In the case of GANs, the generator model applies meaning to points in a chosen latent space, such that new points drawn from the latent space can be provided to the generator model as input and used to generate new and different output examples.\"\n",
        "\n",
        "```\n",
        "\n",
        "### El modelo discriminador\n",
        "El modelo discriminador toma un ejemplo del dominio como entrada (real o generado) y predice una clase binaria con la etiqueta de \"real\" o \"fake\".\n",
        "\n",
        "Los ejemplos reales vienen del training dataset y los generador del modelo generador.\n",
        "\n",
        "El modelo discriminador es basicamente un modelo de clasificación estandar que hace uso de una CNN para la clasificación.\n",
        "\n",
        "### GANs como un juego de dos jugadores\n",
        "\n",
        "Los dos modelos, G y D, se entrenan juntos. El generador crear batches de muestras, y estos, junto a ejemplos reales del dominio se pasan al descriminador para que los clasifique. Despues de esto el discriminador se actualiza para hacer mejores discriminaciones de real y falso en la siguiente ronda, y además, importante, el generador se actualiza basado en como de bien o mal las muestras generadas engañan al discriminador. De esta manera, mirándolo desde una perspectiva de teoría de juegos, los dos modelos serían adversarios jugando a un [juego de suma cero](https://en.wikipedia.org/wiki/Zero-sum_game) Citando a Goodfellows en su tutorial de GANs en el NIPS 2016: *\"Because the GAN framework can naturally be analyzed with the tools of game theory, we call GANs \"adversarial\"\"*\n",
        "\n",
        "En esto caso, suma cero sería cuando el discriminador identifica correctamente las muestras reales  y falsas, y no es necesario realizar actualizaciones en los parametros del generador mientras que el generador es penalizado con acttualizaciones.\n",
        "\n",
        "Alternativamente, cuando el generador miente al discriminador, se premia, o no se necesitan cambios en los parametros del modelo pero el discriminador debe ser penalzado con actualizaciones.\n",
        "\n",
        "Como limite, el generador genera replicas perfectas cada vez y el discriminador no puede encontrar diferencias y predice \"inseguro\" (0.5) en cada caso. Esto es un caso ideal, no es necesario que nuestro modelo llegue a tal punto.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25BshneZOICB",
        "colab_type": "text"
      },
      "source": [
        "# Parte práctica: Implementación en Keras\n",
        "[Libro - Deep Learning with Python.](http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf)\n",
        "\n",
        "[MEDIUM - Generative adversarial networks using Keras](https://medium.com/datadriveninvestor/generative-adversarial-network-gan-using-keras-ce1c05cfdfd3)\n",
        "\n",
        "[Artículo - GAN by Example using Keras on Tensorflow Backend](https://towardsdatascience.com/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0)\n",
        "\n",
        "[Artículo - DCGAN Implementation in Keras explained](https://medium.com/@ramyahrgowda/dcgan-implementation-in-keras-explained-e1918fc930ea)\n",
        "\n",
        "[Artículo científico - Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks](https://arxiv.org/pdf/1511.06434.pdf)\n",
        "\n",
        "Haremos la implementación del GAN en Keras. Concretamente haremos una DCGAN, (DC viene de deep convolutional). Esto es una GAN donde el generador y discriminador son redes convolucionales. Usaremos una capa Conv2DTranspose para hacer upsampling en el generador.\n",
        "\n",
        "Usaremos el Dataset CIFAR10 (**CAMBIAR, MNIST**) pero sólo con la clase \"rana\". Esquemáticamente la GAN se mostrará tal que así:\n",
        "1.   Un *generador* que mapee un vector unidimensional de shape 100 a imágenes (28,28,1).\n",
        "2.   Un *discriminador* que devuelva la probabilidad de que una imagen  sea real.\n",
        "3.   Una red *gan* que una al generador y al discriminador con gan(X) = discriminator(generator(X)). Con esto el gan evaluará el realismo del generador. DUDA\n",
        "4.   Entrenaremos el discriminador usando ejemplos reales y falsos con etiquetas \"real\"/\"fake\" igual que cualquier clasificador de imágenes.\n",
        "5.   Para entrenar el generador, usaremos el gradiente de los pesos del generador respecto a la función pérdida del modelo *gan*. Eso significa, que en cada paso, **modificaremos los pesos del generador en una dirección donde el discriminador clasifica como \"real\" las imagenes decodificadas por el generador**. En otras palabras, entrenamos el generador para engañar al discriminador.\n",
        "\n",
        "### Pequeños tricks\n",
        "\n",
        "El proceso de entrenar las GANs y ajustarlas es dificil. Hay varios truquitos que deberiamos tener en cuenta. \n",
        "\n",
        "*   Usar *tanh* en la última activación del generador en vez de *sigmoid*.** Buscar el por qué.**\n",
        "*   Samplearemos puntos del espacio usando *normal distribution* (Gaussion distribution), no uniform distribution. **Informarse de esto**.\n",
        "*   Estocasticidad es buena para conseguir robustez. Esto es debido a que el resultado del entrenamiento de una GAN es un equilibrio dinámico, las GANs son propensas a atascarse de muchas maneras. Introducir aleatoriedad durante el entrenamineot ayuda a prevenir esto. Esta aleatoriedad la introduciremos de dos maneras: usando **dropout** en el discriminador y añadiendo ruido aleatorio a las etiquetas para el discriminador.\n",
        "\n",
        "*   Operaciones de *MaxPooling* y activaciones *ReLu* pueden producir un gradiente escaso. En vez de max pooling se recomienda usar **strided convolutions** y en vez de activación *ReLu* usar *LeakyRelu*. Esta es parecida a *ReLu* pero estas nunca llegan a cero de tal manera que el gradiente nunca termina de pararse. **BUSCAR INFO**\n",
        "*   En las imagenes generadas es comun ver ciertos artifacts causados por no se cubrir todo el espacio de pixeles en el generador. Para arreglar esto, usaremos un tamaño de kernel que sea divisible por el stride (o paso/salto) cuando usemos strided *Conv2DTranspose* o *Conv2D* tanto en el generador como el discriminador.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iOtfSFO0luk",
        "colab_type": "text"
      },
      "source": [
        "### El generador\n",
        "\n",
        "Primero desarrollaremos el **modelo generador** que transforme un vector (from the latent space during training it will be sampled at random) a una imagen candidata. Uno de los principales problemas de las GANs es que el generador quede atascado con las imágenes generadas que parezcan ruido. Una posible solución es usar dropout tanto en el discriminador como el generador.\n",
        "\n",
        "![Imagen obtenida de https://medium.com/@ramyahrgowda/dcgan-implementation-in-keras-explained-e1918fc930ea](https://miro.medium.com/max/955/1*deJwkHRrQqY2bUfHPG9Ipw.png)\n",
        "\n",
        "El vector de ruido lo metemos a una fully connected layer de 7x7x256. Esta tendrá una salida con función de activación **Leaky ReLU** y funcion de **Batch Normalization**.\n",
        "\n",
        "![Dentro del generador](https://miro.medium.com/max/1364/1*wAb-J_qTptaHDUPufynM2g.png)\n",
        "\n",
        "####¿Cómo aprende el generador?\n",
        "Una de las cosas más interesantes de las GANs es como mejora el generador para crear imágenes más reales. El meollo aquí es que aprende la red para pasar de un vector ruido a una imagen. Esto esta en la fully connected layer, debe aprender a realizar distribuciones semejantes a las de las imágenes reales. **BUSCAR INFORMACIÓN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp2PA5z30oDo",
        "colab_type": "code",
        "outputId": "e68f4141-0219-4d05-e61e-96bf9594de14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Reshape, Dropout, Flatten, Dense, Conv2D, Conv2DTranspose, BatchNormalization, LeakyReLU, Activation\n",
        "import numpy as np\n",
        "\n",
        "generator = Sequential()\n",
        "\n",
        "#Vector ruido a fully connected layer\n",
        "generator.add(Dense(7*7*256, input_dim=100))\n",
        "generator.add(LeakyReLU(alpha=0.2))\n",
        "generator.add(BatchNormalization())\n",
        "generator.add(Reshape((7, 7, 256)))\n",
        "\n",
        "#Primera capa Convolucional transpuesta\n",
        "generator.add(Conv2DTranspose(filters=128, kernel_size=(5,5),\n",
        "                              strides=(1, 1), padding='same'))\n",
        "generator.add(LeakyReLU(alpha=0.2))\n",
        "generator.add(BatchNormalization())\n",
        "\n",
        "#2nda convolución\n",
        "generator.add(Conv2DTranspose(filters=64, kernel_size=(5,5),\n",
        "                              strides=(2, 2), padding='same'))\n",
        "generator.add(LeakyReLU(alpha=0.2))\n",
        "generator.add(BatchNormalization())\n",
        "\n",
        "#3ra convolución\n",
        "generator.add(Conv2DTranspose(filters=1, kernel_size=(5,5),\n",
        "                              strides=(2, 2), padding='same'))\n",
        "generator.add(LeakyReLU(alpha=0.2))\n",
        "generator.add(BatchNormalization())\n",
        "\n",
        "#Capa de activación Tanh\n",
        "generator.add(Activation('tanh'))\n",
        "\n",
        "\n",
        "#Instanciamos el modelo con entrada con shape (latent_dim,) into a imagen de shape (32,32,3)\n",
        "generator.summary()\n",
        "\n",
        "generator.compile(loss='binary_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0807 17:51:34.867038 140038996711296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0807 17:51:34.910676 140038996711296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0807 17:51:34.917330 140038996711296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0807 17:51:35.016533 140038996711296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0807 17:51:35.079842 140038996711296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0807 17:51:37.922165 140038996711296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0807 17:51:38.265992 140038996711296 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0807 17:51:38.289823 140038996711296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 12544)             1266944   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 12544)             50176     \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 7, 7, 128)         819328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 14, 14, 64)        204864    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         1601      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 2,343,685\n",
            "Trainable params: 2,318,211\n",
            "Non-trainable params: 25,474\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a9P9t1-i5Ep",
        "colab_type": "text"
      },
      "source": [
        "Vamos a probar a usar el generador (que aun no está entrenado) para generar una imagen de ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFdzLIEvi_gx",
        "colab_type": "code",
        "outputId": "a29c78df-1539-4147-dfe9-395cd2a16afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "noise = np.random.normal(0,1, (1, 100))\n",
        "imagen_generada = generator.predict(noise, batch_size=1)\n",
        "\n",
        "plt.imshow(imagen_generada[0, :, :, 0], cmap='gray')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5d002c1f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RycG5R1O2wLe",
        "colab_type": "text"
      },
      "source": [
        "### El discriminador\n",
        "\n",
        "Ahora haremos haremos el *discriminador* el cual toma una imagen (real o generada) y la clasifica a una de estas dos calses: \"real image\" o \"generated image\". Paa MNIST tendrá una entrada de (28,28,1). La diferencia de una CNN estandar es la ausencia de max-pooling en las capas intermedias. En vez de esto usaremos las strided convolutions para el downsampling. \n",
        "\n",
        "![texto alternativo](https://miro.medium.com/max/1364/1*QY1ghK_mS_szHkB7GPWO_A.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a-nDjTsjOHR",
        "colab_type": "code",
        "outputId": "ea51679f-4e11-4622-828c-2c012e91708c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        }
      },
      "source": [
        "discriminator = Sequential()\n",
        "\n",
        "#Primera capa convolucional\n",
        "discriminator.add(Conv2D(input_shape = (28,28,1), filters=64,\n",
        "                         kernel_size=(5,5), strides=(2,2), padding='same'))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "discriminator.add(BatchNormalization())\n",
        "\n",
        "#Segunda capa convolucional\n",
        "discriminator.add(Conv2D(input_shape = (14,14,1), filters=128,\n",
        "                         kernel_size=(5,5), strides=(2,2), padding='same'))\n",
        "discriminator.add(LeakyReLU(alpha=0.2))\n",
        "discriminator.add(Dropout(0.3))\n",
        "\n",
        "#Capa flatten\n",
        "discriminator.add(Flatten())\n",
        "\n",
        "#Fully connected layer\n",
        "discriminator.add(Dense(1))\n",
        "\n",
        "discriminator.summary()\n",
        "\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0807 17:51:42.303041 140038996711296 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 64)        1664      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 128)         204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 6273      \n",
            "=================================================================\n",
            "Total params: 213,121\n",
            "Trainable params: 212,993\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtwZJV0d1yAg",
        "colab_type": "text"
      },
      "source": [
        "### Adversarial network\n",
        "\n",
        "Finalmente vamos a preparar el GAN, que une el generador y el discriminador. Entrenar el gan actualizará los pesos del generador de tal manera que haga que el discriminador prediga como \"real\" las imagenes que genera. Es muy importante poner el discriminador como frozen durante el entrenamiento. Sus pesos no deben ser actualizados cuando entrenamos el gan. Si los pesos del discriminador se actualizan durante el proceso, entoncesestariamos entrenando el discriminador para que seimpre prediga como \"real\", y esto no es lo que queremos.\n",
        "\n",
        "**Importante**: \n",
        "\n",
        "Aquí debemos usar la clase Model de Keras. Para la entrada le daremos la entrada del generador y la salida la salida del discriminador."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veMalKo32o-D",
        "colab_type": "code",
        "outputId": "07072720-6c67-4187-c595-bda0d7ec2654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "\n",
        "#Desactivamos entrenamiento para el discriminador\n",
        "discriminator.trainable = False\n",
        "\n",
        "#Unimos D y G para crear la red GAN\n",
        "gan_input = Input(shape=(100,)) #Generamos un tensor para la entrada de la red\n",
        "x = generator(gan_input)\n",
        "gan_output = discriminator(x)\n",
        "gan = Model(inputs=gan_input, outputs=gan_output)\n",
        "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "gan.summary()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 28, 28, 1)         2343685   \n",
            "_________________________________________________________________\n",
            "sequential_2 (Sequential)    (None, 1)                 213121    \n",
            "=================================================================\n",
            "Total params: 2,556,806\n",
            "Trainable params: 2,318,211\n",
            "Non-trainable params: 238,595\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjtbzKrCaQyc",
        "colab_type": "text"
      },
      "source": [
        "###Entrenamiento\n",
        "\n",
        "En el entrenamiento se **entrenan conjuntamente** el modelo generador y discriminador. En cada **epoch** hacemos los siguientes pasos:\n",
        "\n",
        "Para cada **epoch**:\n",
        "\n",
        "  -Para cada **batch**:\n",
        "\n",
        "1.   Entrenamos el discriminador.\n",
        "2.   Entrenamos el generador (es decir, entrenamos el GAN con el entrenamiento del discriminador desactivado).\n",
        "\n",
        "Primero **entrenamos el discriminador** en solitario. Para esto le meteremos dos batches por epoch **batch con imagenes reales y batch con imagenes generadas** con el generador. Las imágenes reales tendrán una etiqueta de 1 y las falsas de 0. De entrenar el discriminador con dos batch de diferentes clases obtendremos dos funciones loss, una para la pérdida que tiene el discriminador con las imagenes reales y otra con los aciertos. Para sacar el loss total haremos la media. Otro posible método sería usar un batch completo 50% real y 50% generado y obtendriamos solo una función loss. Como cualquier CNN estandar vaya.\n",
        "\n",
        "Segundo **entrenamos el GAN** con el aprendizaje en el generador **desactivado**. Es decir, como imagen de entrenamiento usaremos un vector de ruido aleatorio cuya etiqueta será '1' (para que el generador se acerque a aqueellas clasificaciones que el discriminador hace como '1' o 'real') y la salida que dará el GAN serán un valor entre 0 y 1, es decir, la salida que da el discriminador. \n",
        "\n",
        "De esta forma vamos entrenando poco a poco el el generador y el discriminador consiguiendo poco a poco imágenes más parecidas al dominio de problemas.\n",
        "\n",
        "**Importante:**\n",
        "\n",
        "-Para el ruido usamos np.random.normal el cual crea un vector aleatorio con **distribución gaussiana**.\n",
        "\n",
        "-Para hacer un **GIF** con la evolución de las imagenes que crea el GAN y una mejor visualización de como mejora la red crearemos el **vector ruido con una semilla**, de este modo siempre usaremos el mismo.\n",
        "\n",
        "-Para el gif usaremos imageio.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-xA4o08h8r3",
        "colab_type": "code",
        "outputId": "4439f2be-41bc-498f-b7b9-52fdb0117660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from IPython import display\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import PIL\n",
        "\n",
        "display.Image(filename=anim_file)\n",
        "\n",
        "#instalamos imageio\n",
        "!pip install -q imageio\n",
        "\n",
        "#Métodos auxiliares para la representación de imagenes generadas\n",
        "#y la creación del gif para visualizar la mejora de la red en el tiempo\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5766230f3fa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manim_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#instalamos imageio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'anim_file' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KpZAF2Jaj7s",
        "colab_type": "code",
        "outputId": "dada21d1-a0d9-4e3f-ddcc-3623ce9f606e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "\n",
        "epochs = 200\n",
        "batch_size = 32\n",
        "half_batch_size = batch_size//2\n",
        "np.random.seed(1)\n",
        "\n",
        "#Cargamos los datos\n",
        "(x_train, y_train), (_, _) = mnist.load_data()\n",
        "X_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "X_train /= 127.5\n",
        "print(X_train.shape)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #-------------\n",
        "  #DISCRIMINADOR\n",
        "  #-------------\n",
        "\n",
        "  #Creamos un batch para cada clase\n",
        "  noise = np.random.normal(0,1, (half_batch_size, 100))\n",
        "  imagenes_generadas = generator.predict(noise)\n",
        "\n",
        "  #Creamos las etiquetas\n",
        "  fake_labels = np.zeros(half_batch_size) #Etiquetas img generadas\n",
        "  real_labels =np.ones(x_train.shape[0]) #Etiquetas img reales\n",
        "\n",
        "  #Entrenamos y obtenemos los diferentes loss\n",
        "  d_loss_real = discriminator.fit(imagenes_generadas, fake_labels,\n",
        "                              batch_size=half_batch_size)\n",
        "  \n",
        "  d_loss_fake = discriminator.fit(X_train, real_labels,\n",
        "                              batch_size=half_batch_size)\n",
        "  \n",
        "  d_loss = 0.5 * np.add(d_loss_real.history['loss'], d_loss_fake.history['loss'])\n",
        "\n",
        "  #Mostramos el progreso\n",
        "  #print(\"Epoch:\", epoch)\n",
        "  #print(\"loss_real:\", d_loss_real.history['loss'])\n",
        "  #print(\"loss_fake:\", d_loss_real.history['loss'])\n",
        "  #print(\"loss_real:\", d_loss_real.history['loss'])\n",
        "  #print(\"discriminator loss:\", d_loss)\n",
        "\n",
        "  #-------------\n",
        "  #GENERADOR\n",
        "  #-------------  \n",
        "\n",
        "  #Introducimos vector ruido con etiqueta 1\n",
        "  labels = np.array([1]*batch_size)\n",
        "  noise = np.random.normal(0,1, (batch_size, 100)) #32 vectores ruidos (1 epoch) de dimension 100\n",
        "  gan_loss = gan.train_on_batch(noise, labels)\n",
        "\n",
        "  # Plot the progress\n",
        "  #print(\"Epoch:\", epoch)\n",
        "  #print(\"loss_real:\", d_loss_real.history['loss'])\n",
        "  #print(\"loss_fake:\", d_loss_real.history['loss'])\n",
        "  #print(\"loss_real:\", d_loss_real.history['loss'])\n",
        "  #print(\"discriminator loss:\", d_loss)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "Epoch 1/1\n",
            "16/16 [==============================] - 0s 360us/step - loss: 1.0000e-07\n",
            "Epoch 1/1\n",
            "  496/60000 [..............................] - ETA: 19s - loss: 16.1181"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " 2400/60000 [>.............................] - ETA: 17s - loss: 16.1181"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}